---
title: K8S基础操作
date: 2020-06-27 20:05:01
tags:
- k8s
category:
- 物理部署
---
## 1.1 kubectl的命令用法

可以借助kubectl -h命令学习用法，下面介绍常用的一些命令使用：

### 1.1.1 kubectl run，创建一个应用程序

kubectl run nginx-dep --image=nginx:1.7.9 --port=80 --replicas=2 

可以先测试：--dry-run

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg739xsktbj30n201wq3f.jpg)

正式创建：

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73a91do0j30n2034dgi.jpg)

查看服务信息：

kubectl get pods -o wide  

\#获取pod的信息，-o wide 表示更详细的显示信息

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73adfk28j30n201q0t2.jpg) 

看到系统里启动了两个pod服务（运行nginx），分别运行在节点node2和node3上

测试nginx服务，服务ok

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73b1cxv3j30n207uaat.jpg)

### 1.1.2 探究pod详情：

kubectl describe pod nginx-dep-5779c9d6c9-cwjth

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73b6u4rtj30n2084abi.jpg) 

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73bd0iw3j30n20363z7.jpg)

进入容器查看：

```
格式：``kubectl exec` `-it podName -c containerName -n namespace -- shell comand
```

kubectl exec -it nginx-dep-5779c9d6c9-cwjth -c nginx-dep /bin/bash

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73bhabihj30n201ydgd.jpg) 

### 1.1.3 暴露服务到外网

将pod创建完成后，访问该pod内的服务只能在集群内部通过pod的的地址去访问该服务；当该pod出现故障后，该pod的控制器会重新创建一个包括该服务的pod,此时访问该服务须要获取该服务所在的新的pod的地址ip去访问。

\#删除当前的pod:

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73bne7o2j30n2050403.jpg) 

如何保持pod的故障恢复，对调用者无影响呢？

可以创建一个service，当新的pod的创建完成后，service会通过pod的label连接到该服务，只需通过service即可访问该服务。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73bth82tj30n208qmz0.jpg) 

查看svc的label配置

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73c2te4rj30n207mgmq.jpg)

上述方式，虽然能够通过service访问到pod服务。但在集群外部，是无法访问到的，如在本地windows机器上，想要访问到nginx服务，网络是不通的。我们可以修改service的类型的NodePort。

kubectl edit svc nginx-svc

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73c75xhcj30n20c875u.jpg) 

查看绑定端口

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73cew9h5j30n202w74s.jpg) 

在外部可以通过node节点的地址及该端口访问pod内的服务。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73cio6pjj30n207q757.jpg) 

 

### 1.1.4 服务的伸缩

Pod创建完成后，当服务的访问量过大时，可以对pod的进行扩展让pod中的服务处理更多的请求；当访问量减小时，可以缩减pod数量，以节约资源。 这些操作都可以在线完成，并不会影响现有的服务。

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73cmny7vj30n203a3zg.jpg)

缩减服务雷同

 

### 1.1.5 服务的在线升级与回滚

在kubernetes服务中部署完服务后，对服务的升级可以在线完成，升级出问题后，也可以在线完成回滚。、

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73cr465nj30n208s75p.jpg)

可以看到滚动更新的过程：

kubectl get pod -w  ##w参数是watch，持续执行，并观察改变

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73cxdoxtj30n20aiad0.jpg) 

再次查看镜像版本

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73d2uwqpj30n2088q4c.jpg)

还可以再回滚回原来的版本：

kubectl rollout undo deployment nginx-dep

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73d9zfugj30n208u0vb.jpg) 

查看版本，又回到1.7.9的版本

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73dd3bw4j30n208qq4d.jpg)

## 1.2 YAML文件管理资源

K8s两种创建资源的方式：

　　（1）用kubectl命令的方式直接创建：比如前面的创建deployment

　　（2）通过配置文件和kubectl apply创建，

正式的使用，一般采用第二种方式

### 1.2.1 资源清单的格式

apiVersion: group/apiversion # 不指定定group，默认为croe

kind:    #资源类别

metadata： #资源元数据

  name

  namespace #k8s自身的namespace

  lables

  annotations  #主要目的是方便用户阅读查找

spec:期望的状态（disired state）

status：当前状态，本字段有kubernetes自身维护，用户不能去定义

  •缩进表示层级关系

​    •不支持制表符“tab”缩进，使用空格缩进

​    •通常开头缩进2 个空格

​    •字符后缩进1 个空格，如冒号、逗号等

​    •“---” 表示YAML格式，一个文件的开始

  •“#”注释

### 1.2.2 创建deployment

\# vi nginx-deployment.yaml 

apiVersion: apps/v1       # 配置格式的版本      

kind: Deployment                # 创建的资源类型，这里是deployment

metadata:                    # 元数据

 name: nginx-deployment

 namespace: default

spec:

 replicas: 2

 selector:

  matchLabels:

   app: nginx

 template:

  metadata:

   labels:

​    app: nginx

  spec:

   containers:

   \- name: nginx

​    image: nginx:1.7.9

​    ports:

​    \- containerPort: 80

此文件定义内容，效果与上一节中的命令方式相同。但配置更为详尽

```
#使用create 子命令以yaml文件的方式启动
kubectl create -f nginx-deployment.yaml  
 
```

![image-20200627195420189](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73e8d7vlj30n204aq9u.jpg)

### 1.2.3 扩容伸缩

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73ewitxpj30n20aoq40.jpg)

使用kubectl apply生效

kubectl apply -f nginx-deployment.yaml 

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73ezxmbuj30n203emy1.jpg) 

 

### 1.2.4 获取资源的apiVersion版本及资源配置的帮助

```
k8s中， 可以使用kubectl api-versions 获取当前k8s版本上所有的apiVersion版本信息(每个版本可能不同)
```

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73f6rf1uj30fo05cwew.jpg)

```
可以看到出来，不同的资源属于不同的apiVersion版本
```

你还可以查看资源的配置清单的二级级别的字段

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73fctyugj30n207q0tl.jpg)

三级字段

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73fk0kpvj30n205uq3r.jpg) 

### 1.2.5 使用service提供外部访问

vim nginx-service.yaml

apiVersion: v1

kind: Service

metadata:

 name: nginx-service

 labels:

  app: nginx

spec:

 type: NodePort

 ports:

 \- port: 80

  targetPort: 80

 selector:

  app: nginx

创建：kubectl create -f nginx-svc.yaml 

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73fsdj80j30n203kmy0.jpg)

浏览器访问ok

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73fvnkgkj30n20780tk.jpg) 

## 1.3 [Pod控制器(kube-controller-manager)](https://www.cnblogs.com/panwenbin-logs/p/9907847.html)

kube-controller-manager 是Kubernetes 的大脑， 它通过 apiserver 监控整个集群的状态， 并确保集群处于预期的工作状态。

### 1.3.1 ReplicaSets控制器

   k8s中最初有个Replication Controller ，它保证了在所有时间内，都有特定数量的Pod副本正在运行，如果太多了，Replication Controller就杀死几个，如果太少了，Replication Controller会新建几个，和直接创建的pod不同的是，Replication Controller会替换掉那些删除的或者被终止的pod，不管删除的原因是什么（维护阿，更新啊，Replication Controller都不关心）。基于这个理由，我们建议即使是只创建一个pod，我们也要使用Replication Controller。

   Replication Controller 就像一个进程管理器，监管着不同node上的多个pod,而不是单单监控一个node上的pod,Replication Controller 会委派本地容器来启动一些节点上服务（Kubelet ,Docker）。对于服务和用户来说，Replication Controller是通过一种无形的方式来维持着服务的状态

   ReplicaSet是加强版的Replication Controller，其功能只是扩展了Replication Controller的selector，支持基于集合的selector（version in (v1.0, v2.0)或env notin (dev, qa)），K8s中一般我们不再使用Replication Controller。

### 1.3.2 Deployment控制器

   Deployment为Pod和Replica Set（下一代Replication Controller）提供声明式更新。意思就是，Replica Set一是在Deployment里使用的。

你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。

 


 典型的用法：

1.使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。

2.然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。

3.如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。

4.扩容Deployment以满足更高的负载。

5.暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。

6.根据Deployment 的状态判断上线是否hang住了。

7.清除旧的不必要的ReplicaSet。

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73g1pkdej30n209iq3t.jpg)

 

### 1.3.3 DaemonSet

DaemonSet保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。k8s内部就启用了这个，一般运维监控才用到它。日志收集，系统监控等

系统程序，比如kube-proxy, kube-dns, glusterd, ceph等都是DaemonSet

### 1.3.4 StatefulSet

StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），我们后续讲完了K8s存储后，再回头来讲它的使用

### 1.3.5 [Job控制器](https://www.cnblogs.com/caibao666/p/11207677.html)

Job控制器用于调配pod对象运行一次性任务，容器中的进程在正常运行结束后不会对其进行重启，而是将pod对象置于completed状态。若容器中的进程因错误而终止，则需要依据配置确定重启与否，未运行完成的pod对象因其所在的节点故障而意外终止后会被重新调度。

job控制器对象有两种：

Ø 单工作队列的串行式job：即以多个一次性的作业方式串行执行多次作业，直至满足期望的次数

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73ga0ekej30n207omy1.jpg) 

运行后，pod的状态：

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73gjq2zzj30n2042wfd.jpg)

查看pod的运行日志

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73gmt8j3j30n20423zd.jpg)

 

Ø 多工作队列的并行式job：这种方式可以设置工作队列数，即作业数，每个队列仅负责运行一个作业。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73hft8ahj30n2092abd.jpg)

此任务运行后，查看其执行的pod个数

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73hoa5btj30n209qdi4.jpg) 

## 1.4 Pod的调度

   在k8s中当定义某个Pod对象时，若没有特定调度规则设定，则k8s本身会调用GenericScheduler通过预选优选算法来为该Pod选择一个最优Node节点，即最终Node节点是谁是不确定的。例如，我们前面经常用一deployment的过程：

   比如这个创建: 

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73hv4qpgj30n2044gmo.jpg)

   查看其中一个pod的事件列表，第一步都是计算调度的assigned决定：

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73i9g0pjj30n2038gmg.jpg)

   但在实际工作中，我们可能需要指向性地将某个Pod定向调度到某个Node中，K8s为我们准备了几种方式来实现这种需求。

### 1.4.1 NodeName强制约束

我们可以使用NodeName指定，来强制约束pod要在某个node上运行

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73ieeq9ej30n209edh4.jpg)

运行查验

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73iiaok9j30n203ujs5.jpg) 

实际上，这种node的指定是强制的，是直接取消掉的k8s的调度计算的

查看pod的事件列表：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73imloo0j30n202cwem.jpg) 

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73iuwb20j30n2036t9j.jpg)

### 1.4.2 NodeSelector定向调度

   通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是强制约束，使用示例如下：

\1.   给目标node打上一些标签，示例如下：

kubectl label nodes work2 like=north

其实就是为work2节点加一个属性，key和value值都随意

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73j40sm6j30n2044q4d.jpg) 

删除这个属性，只需要将key紧接一个“-”字符即可

kubectl label nodes work2 like-

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73j9t5u6j30n2044gmy.jpg) 

\2.   在pod的定义加上nodeSelector设置（pod的偏向喜好）

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73jfn87jj30n2090gml.jpg)

运行这个pod，看它是否能得偿所愿

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73jx73b5j30n202ijrw.jpg)

### 1.4.3 NodeAffinity调度

   NodeSelector调度算法比较简单，只是调度pod到某个拥有特定标签的Node上。而现实世界复杂的多，我们需要的是一系列的策略来决定，于是NodeAffinity出场，扩展了策略

   NodeAffinity 亲和性有两种表达方式：

Ø RequiredDuringSchedulingIgnoredDuringExecution ：必须满足指定的规则才可以调度Pod到Node上，相当于硬限制。

Ø PreferredDuringSchedulingIgnoredDuringExecution：强调优先满足指定的规则，相当于软限制，并不强求。

示例：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73jp22sbj30n20b0gn0.jpg) 

运行尝试，查验结果

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73kdmvymj30n202ijrv.jpg) 

PreferredDuringSchedulingIgnoredDuringExecution的用法，依次类推，希望你能举一反三。此外，此使用匹配语法灵活，你完全可以配出一系列实际效果，如pod节点的互斥性等等

## 1.5 K8S的label作用

我们在上述的章节中，看到了k8s中一个特殊的存在：label。

   在这里特别强调一下：

   标签是一种简单却又功能强大的kubernetes特性，不仅可以组织pod，也可以组织所有其他的kubernetes资源，标签是可以附加到资源的任意键值对，用以选择具有该确切标签的资源，只要标签的key在资源内是唯一的，一个资源便可以拥有多个标签。

   k8s除了使用标签来控制pod的调度之外，还有两个功能，也是使用label来实现的。

   1.deploy控制器通过label找到对应控制的pods集群

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73khnsgyj30n209i0tt.jpg)

​	2.Service通过label，来绑定port到对应的pods集群，从而提供稳定的服务

 ![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73kobde9j30n2082gmd.jpg)

## 1.6 Pod的健康检查

   Kubernetes会维持Pod的状态及个数，而Pod内容器失败后往往也会导致pod退出，这么看来，K8s在某种程度上已经帮我们保证了容器服务的安全性。但也有很多场景，仅仅这样远远不够，比如某个时候容器服务假死，或者容器服务已经出错但并未退出。这些情况k8s的策略是无能为力的。

   K8S提供了一处机制，来帮助我们来检查容器的健康的程度。

   健康检查（Health Check）用于检测您的应用实例是否正常工作，是保障业务可用性的一种传统机制，一般用于负载均衡下的业务，如果实例的状态不符合预期，将会把该实例“摘除”，不承担业务流量。Kubernetes中的健康检查使用存活性探针（liveness probes）和就绪性探针（readiness probes）来实现，service即为负载均衡，k8s保证 service 后面的 pod 都可用，是k8s中自愈能力的主要手段。

目前支持的探测方式包括：

·    HTTP

·    TCP

·    Exec命令

我们主要讲一下常见易用的Exec方式和Http方式

### 1.6.1 通过exec方式做健康探测

   对于Exec探针，Kubernetes则只是在容器内运行命令。 如果命令以退出代码0返回，则容器标记为健康。 否则，它被标记为不健康。 

   当您不能或不想运行HTTP服务时，此类型的探针则很有用，但是必须是运行可以检查您的应用程序是否健康的命令。

   用法如以下示例：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73lacp16j30n20bg75y.jpg) 

   测试如下：

创建pod

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73lpuwu4j30n2042gmh.jpg)

查看pod事件

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73lv9u6yj30n2048my9.jpg) 

可以看到，在容器刚启动的60秒内，服务是正常的。此时healthy文件也在容器里。但容器启动后60秒后，healthy文件自动被删除，此时liveness检测无法探测到healthy文件，于是触发K8S机制剔除掉服务，并重建容器。

### 1.6.2 通过HTTP方式做健康探测

HTTP探针可能是最常见的自定义Liveness探针类型。 即使您的应用程序不是HTTP服务，您也可以在应用程序内创建轻量级HTTP服务以响应Liveness探针。 Kubernetes去访问一个路径，如果它得到的是200或300范围内的HTTP响应，它会将应用程序标记为健康。 否则它被标记为不健康。

httpGet配置项：

- host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置"Host"而不是使用IP。
- scheme：连接使用的schema，默认HTTP。
- path: 访问的HTTP server的path。
- httpHeaders：自定义请求的header。HTTP运行重复的header。
- port：访问的容器的端口名字或者端口号。端口号必须介于1和65535之间。

示例：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73lzw7b8j30n20f076b.jpg)

 

运行此pod实例

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73m90ehbj30n202274m.jpg) 

查看事件列表

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73mckhqnj30n20420tq.jpg) 

此时，我们执行命令删除掉容器内的文件，看容器事件

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73mgzdwwj30n202k3yw.jpg) 

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gg73ml0yw7j30n2052wg3.jpg)

 

